{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d096e6ef",
   "metadata": {},
   "source": [
    "### 1-Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def load_doc(filename):\n",
    "  file=open(filename,'r')\n",
    "  text=file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "# loading a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "  doc=load_doc(filename)\n",
    "  document=list()\n",
    "  for line in doc.split('\\n'):\n",
    "    if len(line)<1:\n",
    "      continue\n",
    "    identifier= line.split('.')[0]\n",
    "    document.append(identifier)\n",
    "  return set(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90496061",
   "metadata": {},
   "source": [
    "This function adds startseq at the begining and endseq at the end of the sentence,as the model starts its prediction from the start sequence and end its prediction at endseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33084d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename,dataset):\n",
    "  doc=load_doc(filename)\n",
    "  description=dict()\n",
    "  for line in doc.split('\\n'):\n",
    "    token=line.split()\n",
    "    image_id,image_desc=token[0],token[1:]\n",
    "    if image_id in dataset:\n",
    "      if image_id not in description:\n",
    "        description[image_id]=list()\n",
    "      ###Wrapping the desc in the tokens\n",
    "      desc= 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      description[image_id].append(desc)\n",
    "  return description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import *\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the  training dataset (6K)\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f8b4d",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28949491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def to_lines(description):\n",
    "  all_desc=list()\n",
    "  for key in description.keys():\n",
    "    [all_desc.append(d) for d in description[key]]\n",
    "  return all_desc\n",
    "\n",
    "# fiting  a tokenizer given caption descriptions\n",
    "def create_tokenizer(description):\n",
    "  lines=to_lines(description)\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f0f8d9",
   "metadata": {},
   "source": [
    "Given the tokenizer, a maximum sequence length, and the dictionary of all descriptions and photos, will transform the data into input-output pairs of data for training the model. There are two input arrays to the model: one for photo features and one for the encoded text. There is one output for the model which is the encoded next word in the text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7c563c",
   "metadata": {},
   "source": [
    "Maximum length of the sentence in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6367c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f2614",
   "metadata": {},
   "source": [
    "**Defining the Model**\n",
    "\n",
    "We will define a deep learning based on the “merge-model” described by Marc Tanti, et al. in their 2017 papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc12f2",
   "metadata": {},
   "source": [
    "**Model structure**\n",
    "\n",
    "1-Photo Feature Extractor. This is a 16-layer VGG model pre-trained on the ImageNet dataset. We have pre-processed the photos with the VGG model (without the output layer) and will use the extracted features predicted by this model as input.The Photo Feature Extractor model expects input photo features to be a vector of 4,096 elements. These are processed by a Dense layer to produce a 256 element representation of the photo.\n",
    "\n",
    "2-Sequence Processor. This is a word embedding layer for handling the text input, followed by a Long Short-Term Memory (LSTM) recurrent neural network layer.The Sequence Processor model expects input sequences with a pre-defined length (34 words) which are fed into an Embedding layer that uses a mask to ignore padded values. This is followed by an LSTM layer with 256 memory units.\n",
    "\n",
    "3-Decoder-Both the feature extractor and sequence processor output a fixed-length vector. These are merged together and processed by a Dense layer to make a final prediction.The Decoder model merges the vectors from both input models using an addition operation. This is then fed to a Dense 256 neuron layer and then to a final output Dense layer that makes a softmax prediction over the entire output vocabulary for the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.jpg', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f438a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e23706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset (6K)\n",
    "#filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c3415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('/content/drive/MyDrive/Image Captioning/model_' + str(i) + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
