{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1627145241480,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "gDjAaMSbiBYj",
    "outputId": "7ff8cc8a-aee6-4437-e5b2-3dc1e669cd7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 24 16:47:08 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   66C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2160,
     "status": "ok",
     "timestamp": 1627053721726,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "Lwl2eWKexvCs",
    "outputId": "382570d3-cc5d-4769-8299-01ce62053cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.5.0\n",
      "keras: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "import keras\n",
    "print('keras: %s' % keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7icvgN9y-Jw"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKltgX_sLt1G"
   },
   "source": [
    "**Extracting Feature from the image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNc2edyOHKQw"
   },
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "  model=VGG16()\n",
    "  model=Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "  print(model.summary())\n",
    "  features=dict()\n",
    "  for name in listdir(directory):\n",
    "    filname = directory + '/' + name\n",
    "    image = load_img(filname,target_size=(224,224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = model.predict(image,verbose=0)\n",
    "    image_id = name.split('.')[0]\n",
    "    features[image_id] = feature\n",
    "    print('%s' % name)\n",
    "  return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icFEDdXFk78A"
   },
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "\t# load the model\n",
    "\tmodel = VGG16()\n",
    "\t# re-structure the model\n",
    "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t# extract features from each photo\n",
    "\tfeatures = dict()\n",
    "\tfor name in listdir(directory):\n",
    "\t\t# load an image from file\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\timage = load_img(filename, target_size=(224, 224))\n",
    "\t\t# convert the image pixels to a numpy array\n",
    "\t\timage = img_to_array(image)\n",
    "\t\t# reshape data for the model\n",
    "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\t\t# prepare the image for the VGG model\n",
    "\t\timage = preprocess_input(image)\n",
    "\t\t# get features\n",
    "\t\tfeature = model.predict(image, verbose=0)\n",
    "\t\t# get image id\n",
    "\t\timage_id = name.split('.')[0]\n",
    "\t\t# store feature\n",
    "\t\tfeatures[image_id] = feature\n",
    "\t\tprint('>%s' % name)\n",
    "\treturn features\n",
    " \n",
    "# extract features from all images\n",
    "directory = '/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "#dump(features, open('feature.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4j4nRjvxkqS"
   },
   "outputs": [],
   "source": [
    "dump(features, open('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjSAyrWSeDF3"
   },
   "source": [
    "**This Function loads the token file form the given dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1181,
     "status": "ok",
     "timestamp": 1627145256888,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "h2dhxLtMq6Ir"
   },
   "outputs": [],
   "source": [
    "def load_def(filename):\n",
    "  file = open(filename,'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "filename='/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr8k.token.txt'\n",
    "doc= load_def(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87Ahr0_9wDkg"
   },
   "source": [
    "**This function takes the doc.txt file and returns a dictionary with image_id as its key and image_description as its value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1627145262059,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "oyYYpoPNtAwS",
    "outputId": "262b544a-d705-4c97-beef-fabe93225a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092\n"
     ]
    }
   ],
   "source": [
    "def load_description(doc):\n",
    "  mapping=dict()\n",
    "  for line in doc.split('\\n'):\n",
    "    ###Spliting the lines by white space\n",
    "    tokens=line.split()\n",
    "    if len(line)<2:\n",
    "      continue\n",
    "    image_id,image_desc = tokens[0],tokens[1:]\n",
    "    image_id=image_id.split('.')[0]\n",
    "    image_desc=' '.join(image_desc)\n",
    "    if image_id not in mapping:\n",
    "      mapping[image_id]=list()\n",
    "    mapping[image_id].append(image_desc)\n",
    "  return mapping\n",
    "\n",
    "descriptions=load_description(doc)\n",
    "print('Loaded: %d' % len(descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-EasHLU0wue"
   },
   "source": [
    "This Function a basic cleaning of the dataset by doing the following steps\n",
    "\n",
    "\n",
    "1-Convert all words to lowercase\n",
    "\n",
    "2-Remove all the punctuations\n",
    "\n",
    "3-Remove all words that are one character or less in length \n",
    "\n",
    "4-Remove all words with number in them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1627145270527,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "nzbJH7D-0Z7g"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_descriptions(descriptions):\n",
    "  table= str.maketrans('','',string.punctuation)\n",
    "  for key,desc_list in descriptions.items():\n",
    "    for i in range(len(desc_list)):\n",
    "      desc=desc_list[i]\n",
    "      desc=desc.split()\n",
    "      desc=[word.lower() for word in desc]\n",
    "      ###removes punctuation\n",
    "      desc=[w.translate(table) for w in desc]\n",
    "      desc=[word for word in desc if len(word)>1]\n",
    "      ###removes the tokens with number in them\n",
    "      desc=[word for word in desc if word.isalpha()]\n",
    "      desc_list[i]=' '.join(desc)\n",
    "\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1627145274995,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "gyjLMCYc6VGH",
    "outputId": "fc3ae425-c5b5-4772-b522-5df004fd50ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:8763 \n"
     ]
    }
   ],
   "source": [
    "def to_vocabulary(descriptions):\n",
    "  all_desc=set()\n",
    "  for key in descriptions.keys():\n",
    "    [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "  return all_desc\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size:%d '% len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1627145278399,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "3VwRzI8_7Vy5"
   },
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "\tlines = list()\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor desc in desc_list:\n",
    "\t\t\tlines.append(key + ' ' + desc)\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "# save descriptions\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia0-vAx3-BW3"
   },
   "source": [
    "The train and development dataset have been predefined in the Flickr_8k.trainImages.txt and Flickr_8k.devImages.txt files respectively, that both contain lists of photo file names. From these file names, we can extract the photo identifiers and use these identifiers to filter photos and descriptions for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1627145288162,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "uzq5JWAq_DwM"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def load_doc(filename):\n",
    "  file=open(filename,'r')\n",
    "  text=file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "# loading a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "  doc=load_doc(filename)\n",
    "  document=list()\n",
    "  for line in doc.split('\\n'):\n",
    "    if len(line)<1:\n",
    "      continue\n",
    "    identifier= line.split('.')[0]\n",
    "    document.append(identifier)\n",
    "  return set(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDPKZLCUFwwP"
   },
   "source": [
    "This function will take the image id from the load_set and will return a dictonary ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1627145316296,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "F06PqkqeDvBq"
   },
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename,dataset):\n",
    "  doc=load_doc(filename)\n",
    "  description=dict()\n",
    "  for line in doc.split('\\n'):\n",
    "    token=line.split()\n",
    "    image_id,image_desc=token[0],token[1:]\n",
    "    if image_id in dataset:\n",
    "      if image_id not in description:\n",
    "        description[image_id]=list()\n",
    "      ###Wrapping the desc in the tokens\n",
    "      desc= 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      description[image_id].append(desc)\n",
    "  return description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1627145299305,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "TyxYGZcMIQsK"
   },
   "outputs": [],
   "source": [
    "from pickle import *\n",
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4137,
     "status": "ok",
     "timestamp": 1627145340840,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "MjgTKLIiI5n9",
    "outputId": "93185fed-a1fa-49fa-9d72-4e31e01642af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "# loading the  training dataset (6K)\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2688,
     "status": "ok",
     "timestamp": 1627145352874,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "A39ZzlYpVv3u",
    "outputId": "dffc58bb-6f9c-4f45-ad1a-059f83e4e696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def to_lines(description):\n",
    "  all_desc=list()\n",
    "  for key in description.keys():\n",
    "    [all_desc.append(d) for d in description[key]]\n",
    "  return all_desc\n",
    "\n",
    "# fiting  a tokenizer given caption descriptions\n",
    "def create_tokenizer(description):\n",
    "  lines=to_lines(description)\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWH1MbCcgbsQ"
   },
   "source": [
    "There are two input arrays to the model: one for photo features and one for the encoded text. There is one output for the model which is the encoded next word in the text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1627145361798,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "PsiRZikgZh3U"
   },
   "outputs": [],
   "source": [
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1627145368963,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "t-DMPloxjyc-"
   },
   "outputs": [],
   "source": [
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJPzmnf6J--5"
   },
   "source": [
    "**Defining the Model**\n",
    "\n",
    "We will define a deep learning based on the “merge-model” described by Marc Tanti, et al. in their 2017 papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 365,
     "status": "ok",
     "timestamp": 1627145376422,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "HO0XR8-wLVne"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 603,
     "status": "ok",
     "timestamp": 1627145379421,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "2FRon9-vQ7_J"
   },
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tprint(model.summary())\n",
    "\t#plot_model(model, to_file='/content/drive/MyDrive/Image Captioning/model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1627145385796,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "wkKvNuJ9REr4"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "\t\t\tyield [in_img, in_seq], out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1406,
     "status": "ok",
     "timestamp": 1627145400081,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "6Ivba5V3SGhP",
    "outputId": "bedf217d-9b03-4a0e-e992-a308f880ccf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n"
     ]
    }
   ],
   "source": [
    "# load training dataset (6K)\n",
    "#filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4719923,
     "status": "error",
     "timestamp": 1627150125350,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "8Qu-3dOvYdgy",
    "outputId": "17eeb276-6f66-4b90-87b7-67e0c242bddb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 853s 138ms/step - loss: 5.1534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 828s 138ms/step - loss: 3.9077\n",
      "6000/6000 [==============================] - 809s 135ms/step - loss: 3.6505\n",
      "6000/6000 [==============================] - 689s 115ms/step - loss: 3.4839\n",
      "6000/6000 [==============================] - 678s 113ms/step - loss: 3.3765\n",
      "6000/6000 [==============================] - 671s 112ms/step - loss: 3.3052\n",
      " 952/6000 [===>..........................] - ETA: 9:23 - loss: 3.2950"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a85ed22b36f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# fit for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Image Captioning/model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('/content/drive/MyDrive/Image Captioning/model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cshIXxH_fX6O"
   },
   "source": [
    "**Evaluation Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1627150355544,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "rrLLwQo3txqw"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1627150186876,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "YjiZaq7vfXB4"
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    " \n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "\t# seed the generation process\n",
    "\tin_text = 'startseq'\n",
    "\t# iterate over the whole length of the sequence\n",
    "\tfor i in range(max_length):\n",
    "\t\t# integer encode input sequence\n",
    "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pad input\n",
    "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
    "\t\t# predict next word\n",
    "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
    "\t\t# convert probability to integer\n",
    "\t\tyhat = argmax(yhat)\n",
    "\t\t# map integer to word\n",
    "\t\tword = word_for_id(yhat, tokenizer)\n",
    "\t\t# stop if we cannot map the word\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\t# append as input for generating the next word\n",
    "\t\tin_text += ' ' + word\n",
    "\t\t# stop if we predict the end of the sequence\n",
    "\t\tif word == 'endseq':\n",
    "\t\t\tbreak\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1627150191351,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "wR1Vzj0hkRNL"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "\tactual, predicted = list(), list()\n",
    "\t# step over the whole set\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\t# generate description\n",
    "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "\t\t# store actual and predicted\n",
    "\t\treferences = [d.split() for d in desc_list]\n",
    "\t\tactual.append(references)\n",
    "\t\tpredicted.append(yhat.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 410644,
     "status": "ok",
     "timestamp": 1627150776137,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "qIcJAPHvl4CQ",
    "outputId": "99989c9b-f8e5-4db5-9ac4-3bee313c7cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "BLEU-1: 0.540778\n",
      "BLEU-2: 0.286614\n",
      "BLEU-3: 0.191918\n",
      "BLEU-4: 0.081333\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('/content/drive/MyDrive/Image Captioning/Flickr8k_Dataset/feature_new.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = '/content/drive/MyDrive/Image Captioning/model_5.h5'\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RWEp9lkoR6W"
   },
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1627150796990,
     "user": {
      "displayName": "Sanket Mohanty",
      "photoUrl": "",
      "userId": "07259042271467774735"
     },
     "user_tz": -330
    },
    "id": "fryPdsi9oUfS",
    "outputId": "9e8a7586-2cd5-4856-95b1-988bea9e16a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    " \n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    " \n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\t# load document\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# split line by white space\n",
    "\t\ttokens = line.split()\n",
    "\t\t# split id from description\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\t# skip images not in the set\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\t# create list\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\t# wrap description in tokens\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\t# store\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    " \n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    " \n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# load training dataset (6K)\n",
    "filename = '/content/drive/MyDrive/Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/Image Captioning/Flickr8k_text/descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('/content/drive/MyDrive/Image Captioning/tokenizer.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOQmsT+L2sDkNCLOwfs4CMq",
   "collapsed_sections": [],
   "mount_file_id": "1Mw2FkvnTzo3qUk6W8xqkoi2q5hrE3TcM",
   "name": "Captioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
